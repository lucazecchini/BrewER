{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b98b7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "df8929c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking(task, ds, gold):\n",
    "    \n",
    "    # Load the candidate pairs\n",
    "    candidates = pk.load(open(task.candidates, 'rb'))\n",
    "    candidates = set(candidates)\n",
    "    \n",
    "    print(\"Number of candidate pairs generated by blocking: \" + str(len(candidates)) + '\\n', file=open(task.query_output, \"a\"))\n",
    "    \n",
    "    # Measure precision and recall of the blocking method (true positives: intersection between candidates and gold)\n",
    "    tp = gold.intersection(candidates)\n",
    "    print(\"Quality of the blocking:\", file=open(task.query_output, \"a\"))\n",
    "    print(\"TP: \" + str(len(tp)) + \", FP: \" + str(len(gold) - len(tp)) + \" => R: \" + str(\n",
    "        len(tp) / len(gold)) + \", P: \" + str(len(tp) / len(candidates)) + '\\n', file=open(task.query_output, \"a\"))\n",
    "\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6e92447b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolution(ds, cluster, aggregations):\n",
    "    \n",
    "    # Find the matching elements (contained in the cluster cc) in the dataset\n",
    "    matches = ds.loc[ds['id'].isin(cluster)]\n",
    "\n",
    "    # Create the entity (as a dictionary) according to the specified aggregation functions\n",
    "    entity = dict()\n",
    "    for item in aggregations.items():\n",
    "        if item[1] == 'min':\n",
    "            entity[item[0]] = matches[item[0]].min()\n",
    "        elif item[1] == 'max':\n",
    "            entity[item[0]] = matches[item[0]].max()\n",
    "        elif item[1] == 'avg':\n",
    "            entity[item[0]] = round(matches[item[0]].mean(), 2)\n",
    "        elif item[1] == 'sum':\n",
    "            entity[item[0]] = round(matches[item[0]].sum(), 2)\n",
    "        elif item[1] == 'vote':\n",
    "            try:\n",
    "                entity[item[0]] = matches[item[0]].mode(dropna=False).iloc[0]\n",
    "            except ValueError:\n",
    "                entity[item[0]] = matches[item[0]].min()\n",
    "        elif item[1] == 'random':\n",
    "            entity[item[0]] = np.random.choice(matches[item[0]])\n",
    "        elif item[1] == 'concat':\n",
    "            entity[item[0]] = ' ; '.join(matches[item[0]])\n",
    "\n",
    "    return entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97e773ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_er(task, ds, candidates, gold):\n",
    "    \n",
    "    print(\"BATCH ENTITY RESOLUTION ALGORITHM\\n\", file=open(task.query_output, \"a\"))\n",
    "\n",
    "    # Create an empty graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Apply matching function on candidate pairs (verify their presence in gold) and add matches to the graph as edges\n",
    "    for match in gold.intersection(candidates):\n",
    "        graph.add_edge(match[0], match[1])\n",
    "\n",
    "    # Detect clusters (connected components) as sets of nodes and call resolution function on them\n",
    "    entities = list()\n",
    "    for cluster in nx.connected_components(graph):\n",
    "        entities.append(resolution(ds, cluster, task.aggregations))\n",
    "\n",
    "    # Create a new dataset without duplicates\n",
    "    duplicates = ds.loc[ds['id'].isin(list(graph.nodes))]\n",
    "    ds = pd.concat([ds, duplicates], ignore_index=True).drop_duplicates(subset=['id'], keep=False)\n",
    "\n",
    "    # Return the clean dataset obtained by replacing the removed duplicates with the solved entities\n",
    "    return pd.concat([ds, pd.DataFrame(entities)], ignore_index=True).drop_duplicates(subset=['id'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2e8da13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_neighbours(current_id, neighbourhood, neighbours, matches, done, compared, count):\n",
    "    \n",
    "    # Look for the matches among the neighbours\n",
    "    for n in neighbourhood:\n",
    "        \n",
    "        # Do not compare with itself and with elements already inserted in a solved entity or already compared\n",
    "        if n not in matches and n not in done and not compared[n]:\n",
    "            \n",
    "            # Increment the counter of comparisons and register the candidate as compared\n",
    "            count = count + 1\n",
    "            compared[n] = True\n",
    "            \n",
    "            # Apply the matching function\n",
    "            if (current_id, n) in gold or (n, current_id) in gold:\n",
    "                matches.add(n)\n",
    "                matches, compared, count = find_matching_neighbours(n, neighbours[n][0].union(neighbours[n][1]),\\\n",
    "                                                                          neighbours, matches, done, compared, count)\n",
    "    \n",
    "    return matches, compared, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "02fc8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brewer_preliminary_filtering(task, ds, candidates, ol, optimize):\n",
    "    \n",
    "    # Keep a set for the seed records and one for all the records whose block passes the filtering\n",
    "    seeds = list()\n",
    "    filtered = list()\n",
    "    \n",
    "    # From the list of candidate pairs, create the transitively closed blocks\n",
    "    g = nx.Graph()\n",
    "    g.add_nodes_from(list(ds['id']))\n",
    "    g.add_edges_from(candidates)\n",
    "    blocks = list()\n",
    "    for cc in nx.connected_components(g):\n",
    "        blocks.append(list(cc))\n",
    "    \n",
    "    # Perform the preliminary filtering of the transitively closed blocks\n",
    "    for block in blocks:\n",
    "        block_records = ds.loc[ds['id'].isin(block)]\n",
    "        \n",
    "        if len(block) == 1:\n",
    "            solved = True\n",
    "        else:\n",
    "            solved = False\n",
    "        \n",
    "        # Perform preliminary filtering on the records of the block\n",
    "        seed_records = task.brewer_pre_filtering(block_records, solved)\n",
    "        no_seed_records = pd.concat([block_records, seed_records], ignore_index=True).drop_duplicates(subset=['id'], keep=False)\n",
    "        \n",
    "        # Check if the block overcomes the filtering (i.e., if the list of seed records is not empty)\n",
    "        if len(seed_records.index) > 0:\n",
    "            \n",
    "            # If 'ignore_null' option is set, check that at least one record in the block has a not null ordering key value\n",
    "            if task.ignore_null and len(block_records[block_records[task.ordering_key].notnull()]) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                seeds = seeds + list(seed_records['id'])\n",
    "                filtered = filtered + list(seed_records['id']) + list(no_seed_records['id'])\n",
    "                \n",
    "                # If the scenario fits the optimization, insert in OL each record that survives the filtering (seed record)...\n",
    "                # ...otherwise, insert in OL the whole block\n",
    "                if optimize:\n",
    "                    block_records = seed_records\n",
    "                else:\n",
    "                    \n",
    "                    # If 'ignore_null' option is set, insert the records with null ordering key only as neighbours\n",
    "                    # (valid only in general case, not if we apply the optimization)\n",
    "                    if task.ignore_null:\n",
    "                        block_records = block_records[block_records[task.ordering_key].notnull()]\n",
    "                \n",
    "                for index, row in block_records.iterrows():\n",
    "                    \n",
    "                    # Element to be inserted in OL\n",
    "                    element = dict()\n",
    "                    element['id'] = row['id']\n",
    "                    \n",
    "                    # Its attribute 'matches' is a set of identifiers, containing at the moment only the one of the record\n",
    "                    element['matches'] = {row['id']}\n",
    "                    \n",
    "                    # Its attribute 'ordering_key' must be a numeric value (forced cast to float)\n",
    "                    element['ordering_key'] = float(row[task.ordering_key])\n",
    "                    \n",
    "                    element['solved'] = solved\n",
    "                    \n",
    "                    ol.append(element)\n",
    "                    \n",
    "    return ol, seeds, filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "899e00f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brewer(task, ds, gold, candidates):\n",
    "    \n",
    "    # Understand if the current scenario can be optimized or not\n",
    "    optimize = False\n",
    "    if (task.aggregations[task.ordering_key] == 'max' and task.ordering_mode == 'asc') or \\\n",
    "        (task.aggregations[task.ordering_key] == 'min' and task.ordering_mode == 'desc'):\n",
    "        optimize = True\n",
    "    \n",
    "    # Create the ordering list OL\n",
    "    ol = list()\n",
    "    \n",
    "    # Create a dictionary to keep track of all the neighbourhoods\n",
    "    neighbours = dict()\n",
    "    \n",
    "    # Keep a set for the already solved records (entities)\n",
    "    done = set()\n",
    "    \n",
    "    # Initialize OL\n",
    "    ol, seeds, filtered = brewer_preliminary_filtering(task, ds, candidates, ol, optimize)\n",
    "    \n",
    "    seeds = set(seeds)\n",
    "    filtered = set(filtered)\n",
    "    \n",
    "    # Define the neighbours from the list of candidate pairs\n",
    "    for pair in candidates:\n",
    "        if pair[0] in filtered and pair[1] in filtered:\n",
    "        \n",
    "            # If the records are not in neighbours, they must be inserted (a set for seeds, a set for non-seeds)\n",
    "            if pair[0] not in neighbours:\n",
    "                    neighbours[pair[0]] = [set(), set()]\n",
    "            if pair[1] not in neighbours:\n",
    "                    neighbours[pair[1]] = [set(), set()]\n",
    "\n",
    "            if pair[0] in seeds:\n",
    "                neighbours[pair[0]][0].add(pair[0])\n",
    "                neighbours[pair[1]][0].add(pair[0])\n",
    "            else:\n",
    "                neighbours[pair[0]][1].add(pair[0])\n",
    "                neighbours[pair[1]][1].add(pair[0])\n",
    "\n",
    "            if pair[1] in seeds:\n",
    "                neighbours[pair[0]][0].add(pair[1])\n",
    "                neighbours[pair[1]][0].add(pair[1])\n",
    "            else:\n",
    "                neighbours[pair[0]][1].add(pair[1])\n",
    "                neighbours[pair[1]][1].add(pair[1])\n",
    "\n",
    "    with open(task.query_details, 'a') as query_details:\n",
    "        query_details.write(',' + str(len(ol)))\n",
    "        \n",
    "    # Perform progressive entity resolution and count the number of comparisons before each emission\n",
    "    \n",
    "    # Number of comparisons\n",
    "    count = 0\n",
    "    \n",
    "    # List of emitted entities\n",
    "    results = list()\n",
    "    \n",
    "    # Number of emitted entities (for Top-K query case)\n",
    "    top_k_now = 0\n",
    "    \n",
    "    # Initialize a dictionary to keep track of the performed comparisons (for a transitively closed matcher)\n",
    "    compared = {n: False for n in neighbours}\n",
    "    \n",
    "    # At each iteration, order OL and check its first element (priority)\n",
    "    while len(ol) > 0:\n",
    "        \n",
    "        # OL is always kept sorted on the ordering key (according to the specified ordering mode)\n",
    "        if task.ordering_mode == 'asc':\n",
    "            ol = sorted(ol, key=lambda x: x['ordering_key'] if not math.isnan(x['ordering_key']) else float('inf'))\n",
    "        else:\n",
    "            ol = sorted(ol, key=lambda x: x['ordering_key'] if not math.isnan(x['ordering_key']) else float('-inf'),\n",
    "                        reverse=True)\n",
    "        \n",
    "        # If the first element of OL is already solved: perform ER, check HAVING clauses on it...\n",
    "        # ...and eventually emit the entity\n",
    "        if ol[0]['solved']:\n",
    "            \n",
    "            # Perform ER on the records represented by the element (identifiers)\n",
    "            entity = resolution(ds, ol[0]['matches'], task.aggregations)\n",
    "            \n",
    "            # Check HAVING clauses on the entity\n",
    "            if task.brewer_post_filtering(entity):\n",
    "                \n",
    "                # Emit the entity keeping in memory the number of comparisons performed before its emission\n",
    "                entity['comparisons'] = count\n",
    "                results.append(entity)\n",
    "                \n",
    "                # Increment the emitted entities counter and check if it fits the (eventual) K value (Top-K query)...\n",
    "                # ...if it is equal to K, return the result in advance\n",
    "                top_k_now = top_k_now + 1\n",
    "                if top_k_now == task.top_k:\n",
    "                    return pd.DataFrame(results)\n",
    "        \n",
    "            # Remove the considered element from OL\n",
    "            ol.pop(0)\n",
    "        \n",
    "        # If the first element of OL is not solved yet, find the matching neighbours...\n",
    "        # ...and insert in OL a new element representing them\n",
    "        else:\n",
    "            \n",
    "            # Set all the elements in compared to False\n",
    "            compared = dict.fromkeys(compared, False)\n",
    "            \n",
    "            # Look for the matches among the seeds\n",
    "            ol[0]['matches'], compared, count = find_matching_neighbours(ol[0]['id'], neighbours[ol[0]['id']][0], neighbours,\\\n",
    "                                                               ol[0]['matches'], done, compared, count)\n",
    "        \n",
    "            # Check the presence of at least a seed record among the matches...\n",
    "            if len(ol[0]['matches'].intersection(neighbours[ol[0]['id']][0])) > 0:\n",
    "        \n",
    "                # In this case, look for the matches also among the non-seeds...\n",
    "                ol[0]['matches'], compared, count = find_matching_neighbours(ol[0]['id'], neighbours[ol[0]['id']][1],\\\n",
    "                                                                             neighbours, ol[0]['matches'], done, compared,\\\n",
    "                                                                             count)\n",
    "                \n",
    "                # ...and create the representative record:\n",
    "                # The ordering key of the new element is the aggregation of the ones of the matches\n",
    "                key_aggregation = {task.ordering_key: task.aggregations[task.ordering_key]}\n",
    "                entity = resolution(ds, ol[0]['matches'], key_aggregation)\n",
    "                \n",
    "                # Define the new element of OL representing the group of matching elements\n",
    "                solved = dict()\n",
    "                solved['id'] = ol[0]['id']\n",
    "                solved['matches'] = ol[0]['matches']\n",
    "                solved['ordering_key'] = float(entity[task.ordering_key])\n",
    "                del neighbours[ol[0]['id']]\n",
    "                solved['solved'] = True\n",
    "                solved['seed'] = True\n",
    "                \n",
    "                # Insert the matching elements in the list of solved records\n",
    "                done = done.union(ol[0]['matches'])\n",
    "                \n",
    "                # Delete the matching elements from OL\n",
    "                ol = [item for item in ol if item['id'] not in ol[0]['matches']]\n",
    "                \n",
    "                # Insert in OL the new element representing them\n",
    "                ol.append(solved)\n",
    "            \n",
    "            # ...if no seed record is present, delete the current element from OL and insert it in the list of solved records\n",
    "            else:\n",
    "                done = done.union(ol[0]['matches'])\n",
    "                ol = [item for item in ol if item['id'] not in ol[0]['matches']]\n",
    "        \n",
    "    print(\"Total number of performed comparisons: \" + str(count) + '\\n', file=open(task.query_output, \"a\"))\n",
    "\n",
    "    with open(task.query_details, 'a') as query_details:\n",
    "        query_details.write(',' + str(count) + ',' + str(len(results)))\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "836dc9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(object):\n",
    "    \n",
    "    # Identify each instance (query) using an index\n",
    "    def __init__(self, index):\n",
    "        self.counter = index\n",
    "        batch = pd.read_csv(\"results/Experiment 1 (Blocking)/Funding/OR/batch.csv\")\n",
    "\n",
    "        # Choose the dataset to be used (among: alaska_camera, altosight, altosight_sigmod, funding)\n",
    "        self.dataset = 'funding'\n",
    "\n",
    "        # Indicate if the entities with null value of the ordering key must be returned or ignored\n",
    "        self.remove_null = True\n",
    "        if self.remove_null:\n",
    "            self.ds_name = self.dataset + '_no_nan'\n",
    "        else:\n",
    "            self.ds_name = self.dataset\n",
    "\n",
    "        # Define the path of the files to be used to save the results of the query\n",
    "        self.folder_name = \"results/prova\"\n",
    "        self.query_output = self.folder_name + \"/\" + str(self.counter) + \"_query.txt\"\n",
    "        self.brewer_output = self.folder_name + \"/\" + str(self.counter) + \".csv\"\n",
    "        self.query_details = self.folder_name + \"/\" + \"queries.csv\"\n",
    "\n",
    "        # Define if batch version is required\n",
    "        self.batch = True\n",
    "\n",
    "        # SELECT\n",
    "\n",
    "        # Top-K query: define the number of entities (K) to be returned (if all the entities must be returned, set <= 0)\n",
    "        self.top_k = 0\n",
    "\n",
    "        # Indicate if the entities with null value of the ordering key must be returned or ignored\n",
    "        self.ignore_null = True\n",
    "\n",
    "        # Define the aggregation function to be used for each attribute (min, max, avg, sum, vote, random, concat)\n",
    "        # For the ordering key only some aggregation functions (min, max, avg, vote) are supported\n",
    "        # Define also the attributes to be shown for the resulting entities\n",
    "        batch_ok_aggregation = batch['ok_aggregation'].to_list()\n",
    "        random_aggregation = batch_ok_aggregation[index - 1]\n",
    "        # random_aggregation = random.choice(['max', 'min'])\n",
    "        if self.dataset == 'alaska_camera':\n",
    "            self.aggregations = {'id': 'min', 'brand': 'vote', 'model': 'vote', 'megapixels': random_aggregation}\n",
    "            self.attributes = ['brand', 'model', 'megapixels']\n",
    "        elif self.dataset == 'altosight':\n",
    "            self.aggregations = {'id': 'min', 'name': 'vote', 'brand': 'vote', 'size': 'vote', 'size_num': 'max',\n",
    "                             'price': random_aggregation}\n",
    "            self.attributes = ['name', 'brand', 'size', 'size_num', 'price']\n",
    "        elif self.dataset == 'altosight_sigmod':\n",
    "            self.aggregations = {'id': 'min', 'name': 'vote', 'brand': 'vote', 'size': 'vote', 'size_num': 'max',\n",
    "                             'price': random_aggregation}\n",
    "            self.attributes = ['name', 'brand', 'size', 'size_num', 'price']\n",
    "        elif self.dataset == 'funding':\n",
    "            self.aggregations = {'id': 'min', 'legal_name': 'vote', 'address': 'vote', 'source': 'vote',\n",
    "                             'council_member': 'vote', 'amount': random_aggregation}\n",
    "            self.attributes = ['legal_name', 'address', 'source', 'council_member', 'amount']\n",
    "\n",
    "        # FROM\n",
    "\n",
    "        # Define the path of the dataset: it must be a CSV file\n",
    "        self.ds_path = \"data/\" + self.ds_name + \"_dataset.csv\"\n",
    "\n",
    "        # Define the path of the ground truth (a CSV file containing the matching pairs - couples ordered by id)\n",
    "        self.gold_path = \"data/\" + self.ds_name + \"_gold.csv\"\n",
    "\n",
    "        # With blocking or without blocking?\n",
    "        self.blocking = True\n",
    "        self.candidates = \"data/\" + self.ds_name + \"_candidates.pkl\"\n",
    "        # self.blocks_path = \"data/\" + self.ds_name + \"_jedai_blocks.txt\"\n",
    "        # self.block_costs = \"data/\" + self.ds_name + \"_block_costs.txt\"\n",
    "        # self.record_blocks = \"data/\" + self.ds_name + \"_record_blocks.txt\"\n",
    "\n",
    "        # WHERE\n",
    "\n",
    "        # HAVING\n",
    "\n",
    "        # Define HAVING conditions as attribute-value pairs (for LIKE situation)\n",
    "        if self.dataset == 'alaska_camera':\n",
    "            brands = ['canon', 'dahua', 'fuji', 'hikvision', 'kodak', 'nikon', 'olympus', 'panasonic', 'samsung', 'sony']\n",
    "            minor_brands = ['argus', 'benq', 'casio', 'coleman', 'ge', 'gopro', 'hasselblad', 'howell', 'hp', 'intova',\n",
    "                            'leica', 'lg', 'minolta', 'pentax', 'polaroid', 'ricoh', 'sanyo', 'sigma', 'toshiba', 'vivitar']\n",
    "            all_brands = brands + minor_brands\n",
    "            models = {'canon': ['a', 'd', 'elph', 'g', 'ixus', 'mark', 's', 'sd', 'sx', 't', 'xs', 'xt'],\n",
    "                      'dahua': ['dh', 'ipc', 'hd', 'hf', 'sd'], 'fuji': ['ax', 'f', 'hs', 'jx', 's'],\n",
    "                      'hikvision': ['cd', 'de', 'ds', 'f', 'is'], 'kodak': ['dc', 'dx', 'm', 'v', 'z'],\n",
    "                      'nikon': ['100', 'aw', 'd', 'j', 'l', 'p', 's', 'v'],\n",
    "                      'olympus': ['c', 'd', 'e', 'fe', 'sp', 'sz', 'tg', 'vg', 'vr', 'xz'],\n",
    "                      'panasonic': ['dmc', 'fz', 'gf', 'gh', 'gx', 'lx', 'lz', 's', 'tz', 'x', 'z', 'zs'],\n",
    "                      'samsung': ['gc', 'nx', 'pl', 'st', 'wb'],\n",
    "                      'sony': ['tvl', 'a', 'dsc', 'fd', 'pj', 'hx', 'nex', 'slt']}\n",
    "            random_brand = random.choice(brands)\n",
    "            random_model = random.choice(models[random_brand])\n",
    "            batch_cond1 = batch['cond1'].to_list()\n",
    "            batch_cond2 = batch['cond2'].to_list()\n",
    "            random_brand = str(batch_cond1[index - 1])\n",
    "            random_model = str(batch_cond2[index - 1])\n",
    "            self.having = [('brand', random_brand), ('brand', random_model)]\n",
    "        elif self.dataset == 'altosight':\n",
    "            brands = ['intenso', 'kingston', 'lexar', 'pny', 'samsung', 'sandisk', 'sony', 'toshiba', 'transcend']\n",
    "            sizes = ['4', '8', '16', '32', '64', '128', '256', '512']\n",
    "            random_brand = random.choice(brands)\n",
    "            random_size = random.choice(sizes)\n",
    "            batch_cond1 = batch['cond1'].to_list()\n",
    "            batch_cond2 = batch['cond2'].to_list()\n",
    "            random_brand = str(batch_cond1[index - 1])\n",
    "            random_size = str(batch_cond2[index - 1])\n",
    "            self.having = [('brand', random_brand), ('brand', random_size)]\n",
    "        elif self.dataset == 'altosight_sigmod':\n",
    "            brands = ['intenso', 'kingston', 'lexar', 'pny', 'samsung', 'sandisk', 'sony', 'toshiba', 'transcend']\n",
    "            sizes = ['4', '8', '16', '32', '64', '128', '256', '512']\n",
    "            random_brand = random.choice(brands)\n",
    "            random_size = random.choice(sizes)\n",
    "            batch_cond1 = batch['cond1'].to_list()\n",
    "            batch_cond2 = batch['cond2'].to_list()\n",
    "            random_brand = str(batch_cond1[index - 1])\n",
    "            random_size = str(batch_cond2[index - 1])\n",
    "            self.having = [('brand', random_brand), ('brand', random_size)]\n",
    "        elif self.dataset == 'funding':\n",
    "            sources = ['aging', 'aids', 'boro', 'casa', 'food', 'health', 'local', 'youth']\n",
    "            legal_name = ['asian', 'association', 'christian', 'community', 'council', 'foundation', 'jewish', 'service']\n",
    "            random_source = random.choice(sources)\n",
    "            random_name = random.choice(legal_name)\n",
    "            batch_cond1 = batch['cond1'].to_list()\n",
    "            batch_cond2 = batch['cond2'].to_list()\n",
    "            random_source = str(batch_cond1[index - 1])\n",
    "            random_name = str(batch_cond2[index - 1])\n",
    "            self.having = [('source', random_source), ('source', random_name)]\n",
    "\n",
    "        # Define the logical operator to be used for HAVING conditions (and/or)\n",
    "        self.operator = 'or'\n",
    "\n",
    "        # ORDER BY\n",
    "\n",
    "        # Define the numeric attribute to be used as ordering key (OK) and the ordering mode (asc or desc)\n",
    "        if self.dataset == 'alaska_camera':\n",
    "            self.ordering_key = 'megapixels'\n",
    "        elif self.dataset == 'altosight':\n",
    "            self.ordering_key = 'price'\n",
    "        elif self.dataset == 'altosight_sigmod':\n",
    "            self.ordering_key = 'price'\n",
    "        elif self.dataset == 'funding':\n",
    "            self.ordering_key = 'amount'\n",
    "\n",
    "        if random_aggregation == 'max':\n",
    "            self.ordering_mode = 'desc'\n",
    "        else:\n",
    "            self.ordering_mode = 'asc'\n",
    "\n",
    "        # Get the query in SQL\n",
    "        select_clause = \"SELECT \"\n",
    "        if self.top_k > 0:\n",
    "            select_clause = select_clause + \"TOP(\" + str(self.top_k) + \") \"\n",
    "        for i in range(0, len(self.attributes)):\n",
    "            select_clause = select_clause + self.aggregations[self.attributes[i]] + \"(\" + self.attributes[i] + \")\"\n",
    "            if i < len(self.attributes) - 1:\n",
    "                select_clause = select_clause + \", \"\n",
    "            else:\n",
    "                select_clause = select_clause + \"\\n\"\n",
    "        from_clause = \"FROM \" + self.ds_name + \"\\n\"\n",
    "        group_by_clause = \"GROUP BY _\\n\"\n",
    "        having_clause = \"HAVING \"\n",
    "        for i in range(0, len(self.having)):\n",
    "            having_clause = having_clause + self.aggregations[self.having[i][0]] + \\\n",
    "                            \"(\" + str(self.having[i][0]) + \") LIKE '%\" + str(self.having[i][1]) + \"%'\"\n",
    "            if i < len(self.having) - 1:\n",
    "                having_clause = having_clause + \" \" + self.operator + \" \"\n",
    "            else:\n",
    "                having_clause = having_clause + \"\\n\"\n",
    "        order_by_clause = \"ORDER BY \" + self.aggregations[self.ordering_key] + \"(\" + self.ordering_key + \\\n",
    "                          \") \" + self.ordering_mode + \"\\n\"\n",
    "        self.query = (select_clause + from_clause + group_by_clause + having_clause + order_by_clause).upper()\n",
    "\n",
    "    # Define the query for batch ER algorithm\n",
    "    # It is a post filtering (application of HAVING clauses on solved entity), so AND/OR are maintained\n",
    "    def batch_query(self, entities):\n",
    "        if self.ordering_mode == 'asc':\n",
    "            ascending = True\n",
    "        else:\n",
    "            ascending = False\n",
    "        if self.operator == 'and':\n",
    "            return entities.loc[\n",
    "                (entities[self.having[0][0]].str.contains(self.having[0][1], na=False)) &\n",
    "                (entities[self.having[1][0]].str.contains(self.having[1][1], na=False)),\n",
    "                self.attributes].sort_values(by=[self.ordering_key], ascending=ascending)\n",
    "        else:\n",
    "            return entities.loc[\n",
    "                (entities[self.having[0][0]].str.contains(self.having[0][1], na=False)) |\n",
    "                (entities[self.having[1][0]].str.contains(self.having[1][1], na=False)),\n",
    "                self.attributes].sort_values(by=[self.ordering_key], ascending=ascending)\n",
    "\n",
    "    # Define the preliminary filtering for BrewER\n",
    "    def brewer_pre_filtering(self, records, solved):\n",
    "        # To reduce the number of comparisons, if HAVING conditions are in AND...\n",
    "        # ...we check that all conditions are separately satisfied by at least one record appearing the block\n",
    "        if self.operator == 'and':\n",
    "            # If we consider already solved records (no neighbours), simply filter them in AND\n",
    "            if solved:\n",
    "                return records.loc[(records[self.having[0][0]].str.contains(self.having[0][1], na=False)) &\n",
    "                                   (records[self.having[1][0]].str.contains(self.having[1][1], na=False))]\n",
    "            # Otherwise, check that all conditions are separately satisfied (if not, return an empty DataFrame)\n",
    "            else:\n",
    "                condition = records.loc[(records[self.having[0][0]].str.contains(self.having[0][1], na=False))]\n",
    "                if len(condition) == 0:\n",
    "                    return condition\n",
    "                condition = records.loc[(records[self.having[1][0]].str.contains(self.having[1][1], na=False))]\n",
    "                if len(condition) == 0:\n",
    "                    return condition\n",
    "            # If the conditions are all satisfied, proceed as in OR case\n",
    "\n",
    "        # Otherwise, in OR case, check that at least one of the conditions is satisfied by the records of the block\n",
    "        return records.loc[(records[self.having[0][0]].str.contains(self.having[0][1], na=False)) |\n",
    "                           (records[self.having[1][0]].str.contains(self.having[1][1], na=False))]\n",
    "\n",
    "    # Define the post filtering for BrewER (AND/OR are maintained)\n",
    "    def brewer_post_filtering(self, entity):\n",
    "        if self.ignore_null:\n",
    "            if pd.isna(entity[self.ordering_key]):\n",
    "                return False\n",
    "        if self.operator == 'and':\n",
    "            return self.having[0][1] in str(entity[self.having[0][0]]) and \\\n",
    "                   self.having[1][1] in str(entity[self.having[1][0]])\n",
    "        else:\n",
    "            return self.having[0][1] in str(entity[self.having[0][0]]) or \\\n",
    "                   self.having[1][1] in str(entity[self.having[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a65b9f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire the requirements of the task to be performed\n",
    "for query_index in range(1, 2):\n",
    "    task = Task(query_index)\n",
    "    \n",
    "    # Save the query details in the apposite file\n",
    "    if not os.path.isfile(task.query_details):\n",
    "        with open(task.query_details, 'a') as query_details:\n",
    "            query_details.write(\"index,ds_name,top_k,ok,ok_aggregation,ordering_mode,cond1,cond2,operator,\"\n",
    "                                \"ol,tot,emitted\")\n",
    "    with open(task.query_details, 'a') as query_details:\n",
    "        query_details.write('\\n' + str(task.counter) + ',' + task.ds_name + ',' + str(task.top_k) + ',' +\n",
    "                            task.ordering_key + ',' + task.aggregations[task.ordering_key] + ',' +\n",
    "                            task.ordering_mode + ',' + str(task.having[0][1]) + ',' + str(task.having[1][1]) + ',' +\n",
    "                            task.operator)\n",
    "\n",
    "    # Print the query\n",
    "    print(task.query, file=open(task.query_output, \"a\"))\n",
    "\n",
    "    # Load the dataset in DataFrame format, creating also an alternative version as a list of dictionaries\n",
    "    ds = pd.read_csv(task.ds_path)\n",
    "    ds[task.ordering_key] = pd.to_numeric(ds[task.ordering_key], errors='coerce')\n",
    "    for column in ds.columns:\n",
    "        if ds[column].dtype == 'object':\n",
    "            ds[column] = ds[column].fillna('NaN')\n",
    "    ds_dict = ds.to_dict('records')\n",
    "    print(\"Number of records in the dataset: \" + str(len(ds_dict)) + '\\n', file=open(task.query_output, \"a\"))\n",
    "\n",
    "    # Load the ground truth in DataFrame format and transform it into a set of tuples (matching pairs)\n",
    "    gold = pd.read_csv(task.gold_path)\n",
    "    gold = set(list(gold.itertuples(index=False, name=None)))\n",
    "    print(\"Number of matching pairs in ground truth: \" + str(len(gold)) + '\\n', file=open(task.query_output, \"a\"))\n",
    "\n",
    "    # Perform blocking on the dataset\n",
    "    candidates = blocking(task, ds, gold)\n",
    "    \n",
    "    # If required, perform batch ER on the candidate set to get the cleaned dataset (DataFrame composed by entities)\n",
    "    # Then, perform the query on the clean dataset\n",
    "    batch_results = pd.NA\n",
    "    if task.batch:\n",
    "        batch_entities = batch_er(task, ds, candidates, gold)\n",
    "        # If 'ignore null' option is set, ignore the entities with null ordering key\n",
    "        if task.ignore_null:\n",
    "            batch_entities = batch_entities[batch_entities[task.ordering_key].notnull()]\n",
    "        batch_results = task.batch_query(batch_entities)\n",
    "        if len(batch_results.index) > 0:\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "                print(batch_results, file=open(task.query_output, \"a\", encoding=\"utf-8\"))\n",
    "        else:\n",
    "            print(\"No entities satisfied the query\\n\", file=open(task.query_output, \"a\"))\n",
    "    \n",
    "    # Perform progressive ER through BrewER on the dataset\n",
    "    brewer_attributes = task.attributes + ['comparisons']\n",
    "    brewer_results = brewer(task, ds, gold, candidates)\n",
    "\n",
    "    if len(brewer_results.index) > 0:\n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            print(brewer_results.loc[:, brewer_attributes], file=open(task.query_output, \"a\", encoding=\"utf-8\"))\n",
    "            brewer_results.loc[:, brewer_attributes].to_csv(task.brewer_output, index=False)\n",
    "    else:\n",
    "        print(\"No entities satisfied the query\\n\", file=open(task.query_output, \"a\"))\n",
    "    \n",
    "    if task.batch:\n",
    "        # Verify that batch algorithm and BrewER produce the same entities\n",
    "        if len(batch_results.index) > 0 and len(brewer_results.index) > 0:\n",
    "            batch_results = list(batch_results.fillna(0).to_records(index=False))\n",
    "            brewer_results = list(brewer_results[task.attributes].fillna(0).to_records(index=False))\n",
    "            print(\"\\nDifferences in the produced entities between batch algorithm and BrewER: \",\n",
    "                  file=open(task.query_output, \"a\"))\n",
    "            # For Top-K query case, consider only the entities produced by BrewER (batch always exhaustive)\n",
    "            if task.top_k > 0:\n",
    "                print([item for item in brewer_results if item not in batch_results],\n",
    "                      file=open(task.query_output, \"a\", encoding=\"utf-8\"))\n",
    "            else:\n",
    "                print(\n",
    "                    [item for item in batch_results if item not in brewer_results] + [item for item in brewer_results\n",
    "                                                                                     if item not in batch_results],\n",
    "                    file=open(task.query_output, \"a\", encoding=\"utf-8\"))\n",
    "        else:\n",
    "            print(\"One of the two algorithms did not return any entity\\n\", file=open(task.query_output, \"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c02a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
